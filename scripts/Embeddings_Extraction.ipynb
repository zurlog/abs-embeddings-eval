{"cells":[{"cell_type":"markdown","metadata":{},"source":["## **Extracting the Embeddings produced by different PTMs**"]},{"cell_type":"markdown","metadata":{},"source":["### *Notebook Outline*\n","\n","- [**Feature Engineering**](#Features)\n","- [**Casting Data into an HuggingFace Dataset**](#Casting)\n","- [**Getting the Embeddings**](#embeddings)\n","\n","\n","---"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:55:44.034405Z","iopub.status.busy":"2023-08-07T13:55:44.034045Z","iopub.status.idle":"2023-08-07T13:56:16.294217Z","shell.execute_reply":"2023-08-07T13:56:16.293216Z","shell.execute_reply.started":"2023-08-07T13:55:44.034376Z"},"trusted":true},"outputs":[],"source":["# !pip install adapter-transformers\n","from transformers import AutoAdapterModel\n","# %run /kaggle/usr/lib/setup/setup.py\n","%run conf/setup.ipynb\n","%load_ext memory_profiler\n","os.environ['TOKENIZERS_PARALLELISM'] = 'true'"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:16.296582Z","iopub.status.busy":"2023-08-07T13:56:16.296226Z","iopub.status.idle":"2023-08-07T13:56:16.377871Z","shell.execute_reply":"2023-08-07T13:56:16.376833Z","shell.execute_reply.started":"2023-08-07T13:56:16.296551Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 560 entries, 32804639 to 33046370\n","Data columns (total 24 columns):\n"," #   Column                    Non-Null Count  Dtype         \n","---  ------                    --------------  -----         \n"," 0   DOI                       434 non-null    object        \n"," 1   Latest Version            560 non-null    object        \n"," 2   PMCID                     324 non-null    object        \n"," 3   PMID                      368 non-null    float64       \n"," 4   Pub Year                  560 non-null    int64         \n"," 5   Publication Date          560 non-null    datetime64[ns]\n"," 6   Publication Types         560 non-null    object        \n"," 7   Source                    560 non-null    object        \n"," 8   Peer_Review               560 non-null    int64         \n"," 9   Title                     560 non-null    object        \n"," 10  Cleaned_Abs               560 non-null    object        \n"," 11  Lenght_Abs                560 non-null    int64         \n"," 12  Condition                 559 non-null    object        \n"," 13  Total Citations           366 non-null    float64       \n"," 14  Journal Name              367 non-null    object        \n"," 15  Visualization Categories  560 non-null    object        \n"," 16  influence_score           357 non-null    float64       \n"," 17  popularity_alt_score      357 non-null    float64       \n"," 18  popularity_score          357 non-null    float64       \n"," 19  influence_alt_score       357 non-null    float64       \n"," 20  tweets_count              357 non-null    float64       \n"," 21  Data_origin               507 non-null    object        \n"," 22  Task_(primary)            560 non-null    object        \n"," 23  Modality                  560 non-null    object        \n","dtypes: datetime64[ns](1), float64(7), int64(3), object(13)\n","memory usage: 109.4+ KB\n","\n","# of unique PMCID values: 324\n","# of unique PMID values: 368\n","# of unique DOI values: 434\n","# of unique Title values: 560\n"]}],"source":["## KAGGLE\n","# data = pd.read_csv('/kaggle/input/subset-wlabels/subset_wlabels.csv').set_index('System ID')\n","\n","## LOCAL\n"," data = pd.read_csv('subset_wlabels.csv').set_index('System ID')\n","data['Publication Date'] = pd.to_datetime(data['Publication Date'])\n","# Fix missing values coding in the data_origin column\n","data['Data_origin'] = data['Data_origin'].replace('N.A.', pd.NA)\n","data.sort_values(by='Lenght_Abs', inplace=True)\n","\n","data.info()\n","print()\n","print(\"# of unique PMCID values:\", data['PMCID'].nunique())\n","print(\"# of unique PMID values:\", data['PMID'].nunique())\n","print(\"# of unique DOI values:\", data['DOI'].nunique())\n","print(\"# of unique Title values:\", data['Title'].nunique())"]},{"cell_type":"markdown","metadata":{},"source":["### **Feature Engineering** <a id=\"Features\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["In this part of the analysis, we perform various data transformations to enrich our dataset. Let's take a look at the steps:\n","\n","1. **Concatenate Task and Modality**: We create a new label column called \"Task_Modality\" by combining the modified \"Task_(primary)\" and \"Modality\" columns using the string ' with '.\n","\n","2. **Remove Numeric Prefixes**: We remove numeric prefixes from the \"Task_(primary)\" column which were present in the original categorization from Born et al. (2020).\n","\n","3. **Update Task Modality for Reviews**: For rows where \"Task_(primary)\" is equal to 'Review', we replace 'with' with 'on' in the \"Task_Modality\" column.\n","\n","4. **Concatenate Text Fields**: Combine the title and abstract texts into a single sequence, separated by a special token and let the model encode this."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:16.381466Z","iopub.status.busy":"2023-08-07T13:56:16.381179Z","iopub.status.idle":"2023-08-07T13:56:16.423085Z","shell.execute_reply":"2023-08-07T13:56:16.422179Z","shell.execute_reply.started":"2023-08-07T13:56:16.381441Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_28/238933707.py:2: FutureWarning:\n","\n","The default value of regex will change from True to False in a future version.\n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DOI</th>\n","      <th>Latest Version</th>\n","      <th>PMCID</th>\n","      <th>PMID</th>\n","      <th>Pub Year</th>\n","      <th>Publication Date</th>\n","      <th>Publication Types</th>\n","      <th>Source</th>\n","      <th>Peer_Review</th>\n","      <th>Title</th>\n","      <th>...</th>\n","      <th>Visualization Categories</th>\n","      <th>influence_score</th>\n","      <th>popularity_alt_score</th>\n","      <th>popularity_score</th>\n","      <th>influence_alt_score</th>\n","      <th>tweets_count</th>\n","      <th>Data_origin</th>\n","      <th>Task_(primary)</th>\n","      <th>Modality</th>\n","      <th>Task_Modality</th>\n","    </tr>\n","    <tr>\n","      <th>System ID</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>32804639</th>\n","      <td>10.1109/MPULS.2020.3008354</td>\n","      <td>Yes</td>\n","      <td>NaN</td>\n","      <td>32804639.0</td>\n","      <td>2020</td>\n","      <td>2020-08-18</td>\n","      <td>Journal Article</td>\n","      <td>Peer reviewed (PubMed)</td>\n","      <td>1</td>\n","      <td>ai-driven covid-19 tools to interpret quantify...</td>\n","      <td>...</td>\n","      <td>Other Topics</td>\n","      <td>0.000004</td>\n","      <td>49.752</td>\n","      <td>0.000003</td>\n","      <td>185.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>Review</td>\n","      <td>Multimodal</td>\n","      <td>Review on Multimodal</td>\n","    </tr>\n","    <tr>\n","      <th>36237723</th>\n","      <td>10.3348/jksr.2020.0138</td>\n","      <td>Yes</td>\n","      <td>PMC9431829</td>\n","      <td>36237723.0</td>\n","      <td>2020</td>\n","      <td>2020-11-01</td>\n","      <td>English Abstract;Journal Article;Review</td>\n","      <td>Peer reviewed (PubMed)</td>\n","      <td>1</td>\n","      <td>role of chest radiographs and ct scans and the...</td>\n","      <td>...</td>\n","      <td>Polymerase Chain Reaction;Reverse Transcription</td>\n","      <td>0.000002</td>\n","      <td>27.552</td>\n","      <td>0.000002</td>\n","      <td>86.0</td>\n","      <td>-1.0</td>\n","      <td>NaN</td>\n","      <td>Review</td>\n","      <td>Multimodal</td>\n","      <td>Review on Multimodal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows Ã— 26 columns</p>\n","</div>"],"text/plain":["                                  DOI Latest Version       PMCID        PMID  \\\n","System ID                                                                      \n","32804639   10.1109/MPULS.2020.3008354            Yes         NaN  32804639.0   \n","36237723       10.3348/jksr.2020.0138            Yes  PMC9431829  36237723.0   \n","\n","           Pub Year Publication Date                        Publication Types  \\\n","System ID                                                                       \n","32804639       2020       2020-08-18                          Journal Article   \n","36237723       2020       2020-11-01  English Abstract;Journal Article;Review   \n","\n","                           Source  Peer_Review  \\\n","System ID                                        \n","32804639   Peer reviewed (PubMed)            1   \n","36237723   Peer reviewed (PubMed)            1   \n","\n","                                                       Title  ...  \\\n","System ID                                                     ...   \n","32804639   ai-driven covid-19 tools to interpret quantify...  ...   \n","36237723   role of chest radiographs and ct scans and the...  ...   \n","\n","                                  Visualization Categories  influence_score  \\\n","System ID                                                                     \n","32804639                                      Other Topics         0.000004   \n","36237723   Polymerase Chain Reaction;Reverse Transcription         0.000002   \n","\n","          popularity_alt_score popularity_score  influence_alt_score  \\\n","System ID                                                              \n","32804639                49.752         0.000003                185.0   \n","36237723                27.552         0.000002                 86.0   \n","\n","          tweets_count Data_origin  Task_(primary)    Modality  \\\n","System ID                                                        \n","32804639           0.0         NaN          Review  Multimodal   \n","36237723          -1.0         NaN          Review  Multimodal   \n","\n","                  Task_Modality  \n","System ID                        \n","32804639   Review on Multimodal  \n","36237723   Review on Multimodal  \n","\n","[2 rows x 26 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Concatenate the modified \"Task_(primary)\" column with \"Modality\" column using the string ' with '\n","data['Task_Modality'] = (data['Task_(primary)'].str.replace(r'^\\d+\\.\\s*', '') +\n","                         ' with ' +\n","                         data['Modality'])\n","\n","# Remove numeric prefixes from 'Task_(primary)' column\n","data['Task_(primary)'] = data['Task_(primary)'].str.split('.').str[-1].str.strip()\n","\n","# Select rows where \"Task_(primary)\" is equal to 'Review' and replace 'with' with 'on' in \"Task_Modality\" column\n","data.loc[data['Task_(primary)'] == 'Review', 'Task_Modality'] = data[data['Task_(primary)'] == 'Review']['Task_Modality'].str.replace('with', 'on', case=False)\n","\n","# Concatenate Title and Abstract as it is usually done with these texts\n","data.insert(12, 'Title_Abstract', data['Title'] + ' [SEP] ' + data['Cleaned_Abs'])\n","\n","data.head(2)"]},{"cell_type":"markdown","metadata":{},"source":["<br>"]},{"cell_type":"markdown","metadata":{},"source":["### **Casting Data into an HuggingFace Dataset** <a id=\"Casting\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["[`ðŸ¤— Datasets`](https://huggingface.co/docs/datasets/index) supports loading datasets from Pandas DataFrames with the [`from_pandas()`](https://huggingface.co/docs/datasets/tabular_load#pandas-dataframes) method.\n","When the dataset doesnâ€™t look as expected, we should [explicitly specify the dataset features](https://huggingface.co/docs/datasets/loading#specify-features). A `pandas.Series` may not always carry enough information for **Arrow** to automatically infer a data type.\n","\n","[Features](https://huggingface.co/docs/datasets/about_dataset_features) defines the internal structure of a dataset. It is used to specify the underlying serialization format. Whatâ€™s more interesting to you though is that Features contains high-level information about everything from the column names and types, to the ClassLabel. You can think of Features as the backbone of a dataset.\n","The Features format is simple: `dict[column_name, column_type]`. It is a dictionary of column name and column type pairs. The column type provides a wide range of options for describing the type of data you have."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:16.427018Z","iopub.status.busy":"2023-08-07T13:56:16.425497Z","iopub.status.idle":"2023-08-07T13:56:16.434234Z","shell.execute_reply":"2023-08-07T13:56:16.433143Z","shell.execute_reply.started":"2023-08-07T13:56:16.426984Z"},"trusted":true},"outputs":[],"source":["# Columns to encode\n","columns_to_encode = ['Modality', 'Task_Modality', 'Task_(primary)']\n","\n","# Loop through the columns and perform encoding\n","for column in columns_to_encode:\n","    label_encoder = LabelEncoder()\n","    data[column] = label_encoder.fit_transform(data[column])"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:16.435991Z","iopub.status.busy":"2023-08-07T13:56:16.435627Z","iopub.status.idle":"2023-08-07T13:56:16.446974Z","shell.execute_reply":"2023-08-07T13:56:16.445997Z","shell.execute_reply.started":"2023-08-07T13:56:16.435939Z"},"trusted":true},"outputs":[],"source":["modalities = ['X-Ray','CT','Multimodal','Ultrasound']\n","tasks = ['Risk identification','Detection/Diagnosis','Monitoring/Severity assessment','Prognosis/Treatment','Post-hoc','Segmentation-only','Review']\n","task_modalities = ['Detection/Diagnosis with X-Ray','Detection/Diagnosis with CT','Monitoring/Severity assessment with CT','Segmentation-only with CT','Detection/Diagnosis with Multimodal','Review on Multimodal','Prognosis/Treatment with CT','Prognosis/Treatment with X-Ray','Monitoring/Severity assessment with X-Ray','Segmentation-only with X-Ray','Detection/Diagnosis with Ultrasound','Review on X-Ray','Post-hoc with X-Ray','Monitoring/Severity assessment with Multimodal','Review on CT','Risk identification with CT','Post-hoc with CT','Segmentation-only with Multimodal','Monitoring/Severity assessment with Ultrasound']\n","\n","dataset_features = Features({'DOI': Value(dtype='string'),\n","'PMCID': Value(dtype='string'),\n","'PMID': Value(dtype='float64'),\n","'Publication Types': Value(dtype='string'),\n","'Title': Value(dtype='string'),\n","'Title_Abstract': Value(dtype='string'),\n","'Task_(primary)': ClassLabel(names=tasks),\n","'Modality': ClassLabel(names=modalities),\n","'Task_Modality': ClassLabel(names=task_modalities),\n","'System ID': Value(dtype='string')})"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:16.448563Z","iopub.status.busy":"2023-08-07T13:56:16.448225Z","iopub.status.idle":"2023-08-07T13:56:16.485402Z","shell.execute_reply":"2023-08-07T13:56:16.484388Z","shell.execute_reply.started":"2023-08-07T13:56:16.448530Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'DOI': Value(dtype='string', id=None),\n"," 'PMCID': Value(dtype='string', id=None),\n"," 'PMID': Value(dtype='float64', id=None),\n"," 'Publication Types': Value(dtype='string', id=None),\n"," 'Title': Value(dtype='string', id=None),\n"," 'Title_Abstract': Value(dtype='string', id=None),\n"," 'Task_(primary)': ClassLabel(num_classes=7, names=['Risk identification', 'Detection/Diagnosis', 'Monitoring/Severity assessment', 'Prognosis/Treatment', 'Post-hoc', 'Segmentation-only', 'Review'], id=None),\n"," 'Modality': ClassLabel(num_classes=4, names=['X-Ray', 'CT', 'Multimodal', 'Ultrasound'], id=None),\n"," 'Task_Modality': ClassLabel(num_classes=19, names=['Detection/Diagnosis with X-Ray', 'Detection/Diagnosis with CT', 'Monitoring/Severity assessment with CT', 'Segmentation-only with CT', 'Detection/Diagnosis with Multimodal', 'Review on Multimodal', 'Prognosis/Treatment with CT', 'Prognosis/Treatment with X-Ray', 'Monitoring/Severity assessment with X-Ray', 'Segmentation-only with X-Ray', 'Detection/Diagnosis with Ultrasound', 'Review on X-Ray', 'Post-hoc with X-Ray', 'Monitoring/Severity assessment with Multimodal', 'Review on CT', 'Risk identification with CT', 'Post-hoc with CT', 'Segmentation-only with Multimodal', 'Monitoring/Severity assessment with Ultrasound'], id=None),\n"," 'System ID': Value(dtype='string', id=None)}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dataset = Dataset.from_pandas(data[['DOI','PMCID','PMID','Publication Types','Title','Title_Abstract',\n","                                     'Task_(primary)','Modality','Task_Modality']], features = dataset_features)\n","dataset.features"]},{"cell_type":"markdown","metadata":{},"source":["<br>"]},{"cell_type":"markdown","metadata":{},"source":["### **Getting the Embeddings** <a id=\"embeddings\"></a>\n","\n","We begin by instantiating the tokenizer for each candidate model. 15 BERT variants were selected for comparison, and we select one at a time by uncommenting the corresponding checkpoint identifier. The models considered in this study share identical architectures, namely `bertbase` or `bertlarge`. However, their distinctions lie in three key aspects: the pre-training dataset, weight initialization, and the vocabulary. \n","\n","The tokenizer is initialized using the selected model checkpoint, and we manually set the maximum token length to 512, as it may default to a very large value by default (int(1e30)). This ensures that the input text is appropriately tokenized and fits within the model's constraints.\n","\n","[PreTrainedTokenizerFast](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) - Base Class for all fast Tokenizers. <br>\n","[BertTokenizerFast](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizerFast) - Subclass in use here."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:16.487411Z","iopub.status.busy":"2023-08-07T13:56:16.487056Z","iopub.status.idle":"2023-08-07T13:56:18.163862Z","shell.execute_reply":"2023-08-07T13:56:18.162679Z","shell.execute_reply.started":"2023-08-07T13:56:16.487370Z"},"trusted":true},"outputs":[],"source":["# Instantiate the tokenizer for each candidate model\n","%memit\n","\n","#model_ckpt = 'bert-base-uncased'\n","#model_ckpt = 'allenai/scibert_scivocab_uncased'\n","#model_ckpt = 'dmis-lab/biobert-base-cased-v1.2'\n","#model_ckpt = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n","#model_ckpt = 'deepset/covid_bert_base'\n","#model_ckpt = 'lordtt13/COVID-SciBERT'\n","#model_ckpt = 'manueltonneau/clinicalcovid-bert-base-cased'\n","#model_ckpt = 'StanfordAIMI/RadBERT'\n","#model_ckpt = 'allenai/specter2'\n","\n","model_ckpt = 'bert-large-cased' # LARGE\n","#model_ckpt = 'dmis-lab/biobert-large-cased-v1.1' # LARGE\n","#model_ckpt = 'microsoft/BiomedNLP-PubMedBERT-large-uncased-abstract'\n","#model_ckpt = 'manueltonneau/biocovid-bert-large-cased' # LARGE\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt, model_max_length=512)\n","# Manually setting model_max_length as it may default to VERY_LARGE_INTEGER (int(1e30)) \n","tokenizer.init_kwargs"]},{"cell_type":"markdown","metadata":{},"source":["The `max_model_input_sizes` Class attribute: A dictionary with, as keys, the short-cut-names of the pretrained models using the tokenizer, and as associated values, the maximum length of the sequence inputs of this model, or `None` if the model has no maximum input size.\n","Checking this attribute is useful for manually setting `model_max_length` as it defaults to `VERY_LARGE_INTEGER` (*int(1e30)*).\n"," <br>\n"," \n","  **Note:** Not setting this parameter will force the tokenizer to ignore the `Truncation = True` argument in the function call.  "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:18.166589Z","iopub.status.busy":"2023-08-07T13:56:18.165985Z","iopub.status.idle":"2023-08-07T13:56:18.174684Z","shell.execute_reply":"2023-08-07T13:56:18.173662Z","shell.execute_reply.started":"2023-08-07T13:56:18.166551Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'bert-base-uncased': 512,\n"," 'bert-large-uncased': 512,\n"," 'bert-base-cased': 512,\n"," 'bert-large-cased': 512,\n"," 'bert-base-multilingual-uncased': 512,\n"," 'bert-base-multilingual-cased': 512,\n"," 'bert-base-chinese': 512,\n"," 'bert-base-german-cased': 512,\n"," 'bert-large-uncased-whole-word-masking': 512,\n"," 'bert-large-cased-whole-word-masking': 512,\n"," 'bert-large-uncased-whole-word-masking-finetuned-squad': 512,\n"," 'bert-large-cased-whole-word-masking-finetuned-squad': 512,\n"," 'bert-base-cased-finetuned-mrpc': 512,\n"," 'bert-base-german-dbmdz-cased': 512,\n"," 'bert-base-german-dbmdz-uncased': 512,\n"," 'TurkuNLP/bert-base-finnish-cased-v1': 512,\n"," 'TurkuNLP/bert-base-finnish-uncased-v1': 512,\n"," 'wietsedv/bert-base-dutch-cased': 512}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.max_model_input_sizes"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:18.176901Z","iopub.status.busy":"2023-08-07T13:56:18.176196Z","iopub.status.idle":"2023-08-07T13:56:18.185673Z","shell.execute_reply":"2023-08-07T13:56:18.184518Z","shell.execute_reply.started":"2023-08-07T13:56:18.176859Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary Size: 28996\n","Context Window Size: 512\n","Model Input Fields: ['input_ids', 'token_type_ids', 'attention_mask']\n"]}],"source":["print('Vocabulary Size: ' + str(tokenizer.vocab_size))\n","print('Context Window Size: ' + str(tokenizer.model_max_length)) # Enough for papers' abstracts\n","print('Model Input Fields: ' + str(tokenizer.model_input_names)) # Names of the fields that the model expects in its forward pass"]},{"cell_type":"markdown","metadata":{},"source":["We now test the tokenization process using an individual record from the dataset. We are using the tokenizer to encode the text of the \"Title_Abstract\" column. \\\n","To observe the impact of padding, set the padding strategy to `max_length`, which will pad the sequences to the maximum length.\\\n","To observe the impact of padding, tokenize the last record, which originally has 619 tokens, but it will be truncated to 512 tokens."]},{"cell_type":"code","execution_count":10,"metadata":{"_kg_hide-output":false,"execution":{"iopub.execute_input":"2023-08-07T13:56:18.190177Z","iopub.status.busy":"2023-08-07T13:56:18.189909Z","iopub.status.idle":"2023-08-07T13:56:18.208217Z","shell.execute_reply":"2023-08-07T13:56:18.206838Z","shell.execute_reply.started":"2023-08-07T13:56:18.190155Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [101, 170, 1182, 118, 4940, 1884, 18312, 118, 1627, 5537, 1106, 19348, 186, 27280, 6120, 13093, 4351, 102, 154, 4746, 24936, 7628, 1110, 170, 1363, 1645, 1165, 1122, 2502, 1106, 3455, 13093, 4351, 1107, 1103, 2147, 1222, 1884, 15789, 27608, 10351, 3653, 113, 18732, 23314, 2137, 118, 1627, 114, 117, 1133, 25220, 3622, 2228, 2070, 6360, 7516, 1277, 1167, 8232, 119, 1706, 1115, 1322, 117, 1317, 1844, 2114, 1138, 4972, 1702, 1106, 8246, 4810, 113, 19016, 114, 1112, 170, 6806, 1111, 3455, 1105, 23389, 161, 118, 11611, 1105, 3254, 18505, 1106, 3702, 8944, 113, 16899, 114, 14884, 1116, 117, 1105, 4395, 1106, 4267, 8517, 22583, 1105, 8804, 18732, 23314, 2137, 118, 1627, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","\n","Length of a generic individual encoded record 114\n"]}],"source":["# Testing Tokenization with an individual record\n","# Check out padding by setting padding strategy to 'max_length'\n","# Check out Truncation by tokenizing the last records (619 tokens truncated to 512)\n","encoded_text = tokenizer(dataset['Title_Abstract'][0], padding=True, truncation=True)\n","\n","print(encoded_text) # transformers.tokenization_utils_base.BatchEncoding\n","print()\n","print('Length of a generic individual encoded record ' + str(len(encoded_text['input_ids'])))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:18.209727Z","iopub.status.busy":"2023-08-07T13:56:18.209404Z","iopub.status.idle":"2023-08-07T13:56:18.214537Z","shell.execute_reply":"2023-08-07T13:56:18.213702Z","shell.execute_reply.started":"2023-08-07T13:56:18.209698Z"},"trusted":true},"outputs":[],"source":["# convert the token IDs back to their corresponding string tokens\n","#tokenizer.convert_ids_to_tokens(encoded_text.input_ids)"]},{"cell_type":"markdown","metadata":{},"source":["<br>"]},{"cell_type":"markdown","metadata":{},"source":["Each of the models used here will be a [`BertModel`]([transformers.models.bert.modeling_bert.BertModel](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)) instance.\n","\n","The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\n","This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model *(such as downloading or saving, resizing the input embeddings, pruning heads etc.)*\n","\n","The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of cross-attention is added between the self-attention layers, following the architecture described in *Attention is all you need*\n","\n","To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and `add_cross_attention` set to True; an `encoder_hidden_states` is then expected as an input to the forward pass."]},{"cell_type":"markdown","metadata":{},"source":["> In the Jupyter notebook, the `get_embeddings()` function plays a crucial role in addressing errors related to the scope of global variables within the `tokenize()` and `extract_hidden_state()` functions, as surprisingly raised in Kaggle's Kernel. By encapsulating the instantiation of the tokenizer and associated models within the `get_embeddings()` function, we ensure convenient and localized access to these resources for every row. It's important to note that the function should be executed without batching, indicated by setting `batch_size = None`. If batching is used, the model would be loaded multiple times, which is inefficient. Please be aware that the **batch mapping feature is currently under development** and not fully functional at this stage.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:18.216904Z","iopub.status.busy":"2023-08-07T13:56:18.215920Z","iopub.status.idle":"2023-08-07T13:56:18.227300Z","shell.execute_reply":"2023-08-07T13:56:18.226347Z","shell.execute_reply.started":"2023-08-07T13:56:18.216873Z"},"trusted":true},"outputs":[],"source":["# embeddings = dataset.map(get_embeddings, fn_kwargs={\"model_ckpt\": model_ckpt}, batched=True, batch_size=None)\n","# embeddings.features"]},{"cell_type":"markdown","metadata":{},"source":["We initialize and set up the candidate model from which exctracting the embeddings. We check for the availability of CUDA and assign the appropriate device (GPU if available, else CPU).\n","We then instantiate the model using `AutoModel.from_pretrained()` with the specified checkpoint (`model_ckpt`) and move it to the available device.\n","Please note that the commented blocks at the bottom demonstrate how to load the SPECTER model with adapters for specific tasks, if needed."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:18.229360Z","iopub.status.busy":"2023-08-07T13:56:18.228922Z","iopub.status.idle":"2023-08-07T13:56:34.478681Z","shell.execute_reply":"2023-08-07T13:56:34.476812Z","shell.execute_reply.started":"2023-08-07T13:56:18.229327Z"},"trusted":true},"outputs":[],"source":["# Check if CUDA is available, and assign the device accordingly\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate each candidate model\n","model = AutoModel.from_pretrained(model_ckpt).to(device)\n","\n","# Instantiate SPECTER model with adapter\n","#model = AutoModel.from_pretrained(model_ckpt)\n","# load the adapter(s) as per the required task, provide an identifier for the adapter in load_as argument and activate it\n","#model.load_adapter(\"allenai/specter2_proximity\", source=\"hf\", set_active=True)\n","#model.to(device)\n","print(\"running on device: {}\".format(device))"]},{"cell_type":"markdown","metadata":{},"source":["In this cell, we generate text embeddings using a selected pre-trained BERT variant model. We derive the hidden size from the number of inputs attribute of the pooler layer.\n","\n","The process involves iterating through the dataset's 'Title_Abstract' column and obtaining embeddings for each abstract using the specified tokenizer and model. The embeddings are stored in three separate arrays, each representing the `CLS` token, `SEP` token, and the pooled representation of the abstract, respectively.\n","\n","Finally, the generated embeddings are added as new columns in the pandas DataFrame with corresponding names, like `[model_name]_CLS_embed`, `[model_name]_SEP_embed`, and `[model_name]_POOL_embed`. \\\n"," These are then saved as Pandas Series in a serialized .pkl file format.\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:56:34.488523Z","iopub.status.busy":"2023-08-07T13:56:34.487960Z","iopub.status.idle":"2023-08-07T13:57:07.045175Z","shell.execute_reply":"2023-08-07T13:57:07.044032Z","shell.execute_reply.started":"2023-08-07T13:56:34.488492Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["peak memory: 4028.95 MiB, increment: 0.00 MiB\n","CPU times: user 29.4 s, sys: 236 ms, total: 29.6 s\n","Wall time: 31.2 s\n"]}],"source":["%%time\n","%memit\n","\n","# One batch extraction no longer works with the 517 records dataset\n","#embeddings = generate_embeddings(dataset['Title_Abstract'], tokenizer, model, device)\n","\n","hidden_size = model.pooler.dense.in_features\n","n_rows = dataset.num_rows\n","\n","## UNCOMMENT IF MODEL IS A BERT MODEL (768-dim Embeddings)\n","embeddings = (np.zeros([n_rows, hidden_size]), np.zeros([n_rows, hidden_size]), np.zeros([n_rows, hidden_size]))\n","for i, abst in enumerate(dataset['Title_Abstract']): \n","    embeddings[0][i], embeddings[1][i], embeddings[2][i] = generate_embeddings(abst, tokenizer, model, device)\n","\n","model_name = model_ckpt.split(\"/\")[-1]\n","\n","# Optionally save individual arrays to Numpy standard binary format .npy\n","#np.save(model_name + '_CLS_embed', embeddings[0])\n","#np.save(model_name + '_SEP_embed', embeddings[1]) \n","#np.save(model_name + '_POOL_embed', embeddings[2]) \n","\n","# Create new columns in the pandas DataFrame and assign the embeddings\n","data[model_name + '_CLS_embed'] = embeddings[0].tolist()\n","data[model_name + '_SEP_embed'] = embeddings[1].tolist()\n","data[model_name + '_POOL_embed'] = embeddings[2].tolist()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:57:07.047189Z","iopub.status.busy":"2023-08-07T13:57:07.046816Z","iopub.status.idle":"2023-08-07T13:57:07.124952Z","shell.execute_reply":"2023-08-07T13:57:07.124003Z","shell.execute_reply.started":"2023-08-07T13:57:07.047151Z"},"trusted":true},"outputs":[],"source":["# Save CLS embeddings and keys to a serialized file\n","data[model_name + '_CLS_embed'].to_pickle(model_name + '_CLS_embed.pkl')\n","\n","# Save SEP embeddings and keys to a serialized file\n","data[model_name + '_SEP_embed'].to_pickle(model_name + '_SEP_embed.pkl')\n","\n","# Save POOL embeddings and keys to a serialized file\n","data[model_name + '_POOL_embed'].to_pickle(model_name + '_POOL_embed.pkl')"]},{"cell_type":"markdown","metadata":{},"source":["***"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
